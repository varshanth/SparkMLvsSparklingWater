{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_susy_into_df import susy_csv_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = '/home/varsrao/Downloads/SUSY.csv'\n",
    "chunksize = 100000\n",
    "num_train_chunks = 5\n",
    "num_test_chunks = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Loading Dataset----\n"
     ]
    }
   ],
   "source": [
    "print('----Loading Dataset----')\n",
    "ds_train_pd_df, ds_test_pd_df, target_col_name, target_col_idx, feature_col_names = susy_csv_to_df(\n",
    "        path_to_csv, chunksize, num_train_chunks, num_test_chunks)\n",
    "col_names = [target_col_name]+feature_col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Creating Spark Context----\n"
     ]
    }
   ],
   "source": [
    "print('----Creating Spark Context----')\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "ds_spark_df = sqlCtx.createDataFrame(ds_train_pd_df, schema=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Assembling Data----\n"
     ]
    }
   ],
   "source": [
    "print('----Assembling Data----')\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecassembler = VectorAssembler(\n",
    "        inputCols=ds_spark_df.columns[:target_col_idx]+ds_spark_df.columns[target_col_idx+1:],\n",
    "        outputCol=\"features\")\n",
    "features_vec = vecassembler.transform(ds_spark_df)\n",
    "\n",
    "features_vec = features_vec.withColumnRenamed(target_col_name, \"label\")\n",
    "features_data = features_vec.select(\"label\", \"features\")\n",
    "feat_train, feat_test = features_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training Model----\n"
     ]
    }
   ],
   "source": [
    "print('----Training Model----')\n",
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=10, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(feat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Testing Model----\n",
      "[0.2143888254758566,0.14685310500965998,0.12659308246789863,0.11748130061622955,0.09897783738723055,0.09276907508343527,0.059203042294301435,0.0534092724470162,0.031717478230009435,0.019616299925438483]\n",
      "----End----\n"
     ]
    }
   ],
   "source": [
    "print('----Testing Model----')\n",
    "pca_model.transform(feat_test).collect()[0].pca_features\n",
    "print(pca_model.explainedVariance)\n",
    "\n",
    "\n",
    "print('----End----')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
