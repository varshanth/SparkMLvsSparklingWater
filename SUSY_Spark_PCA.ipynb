{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_susy_into_df import susy_csv_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = '/home/varsrao/Downloads/SUSY.csv'\n",
    "chunksize = 100000\n",
    "num_train_chunks = 4\n",
    "num_test_chunks = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Loading Dataset----\n"
     ]
    }
   ],
   "source": [
    "print('----Loading Dataset----')\n",
    "ds_train_pd_df, ds_test_pd_df, target_col_name, target_col_idx, feature_col_names = susy_csv_to_df(\n",
    "        path_to_csv, chunksize, num_train_chunks, num_test_chunks)\n",
    "col_names = [target_col_name]+feature_col_names\n",
    "import pandas as pd\n",
    "ds_merged_pd_df = pd.concat([ds_train_pd_df, ds_test_pd_df])\n",
    "train_frac = 1. * num_train_chunks/(num_test_chunks+num_train_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Creating Spark Context----\n"
     ]
    }
   ],
   "source": [
    "print('----Creating Spark Context----')\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Creating Spark DataFrame----\n"
     ]
    }
   ],
   "source": [
    "print('----Creating Spark DataFrame----')\n",
    "ds_spark_df = sqlCtx.createDataFrame(ds_merged_pd_df, schema=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Assembling Data----\n"
     ]
    }
   ],
   "source": [
    "print('----Assembling Data----')\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecassembler = VectorAssembler(\n",
    "        inputCols=ds_spark_df.columns[:target_col_idx]+ds_spark_df.columns[target_col_idx+1:],\n",
    "        outputCol=\"features\")\n",
    "features_vec = vecassembler.transform(ds_spark_df)\n",
    "features_vec = features_vec.withColumnRenamed(target_col_name, \"label\")\n",
    "features_data = features_vec.select(\"label\", \"features\")\n",
    "feat_train, feat_test = features_data.randomSplit([train_frac, 1-train_frac])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training Model----\n"
     ]
    }
   ],
   "source": [
    "print('----Training Model----')\n",
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=10, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(feat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Testing Model----\n",
      "[0.21485667672346023,0.14648695388309613,0.12660475175565475,0.11747758332585373,0.09909941980142006,0.09268735569123193,0.0592136181969622,0.05332737618977345,0.03169908737948333,0.01957131470991901]\n",
      "----End----\n"
     ]
    }
   ],
   "source": [
    "print('----Testing Model----')\n",
    "pca_model.transform(feat_test).collect()[0].pca_features\n",
    "print(pca_model.explainedVariance)\n",
    "\n",
    "\n",
    "print('----End----')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
